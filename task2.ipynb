{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}, {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained LLaVA-Next model and processor\n",
    "model_name = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "processor = LlavaNextProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Load and configure the model\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(device)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XRayReportDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_dir: str, \n",
    "                 annotation_file: str, \n",
    "                 split: str = 'train'):\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.processor = processor  # Use LlavaNextProcessor\n",
    "\n",
    "        # Load annotations\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            self.annotations = json.load(f)[split]\n",
    "\n",
    "        # Define anatomical regions\n",
    "        self.regions = ['lung', 'heart', 'mediastinal', 'bone']\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        annotation = self.annotations[idx]\n",
    "        patient_id = annotation['id']\n",
    "        report = annotation['report']\n",
    "\n",
    "        # Load the first image for this patient\n",
    "        image_folder = os.path.join(self.data_dir, patient_id)\n",
    "        image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.png')])\n",
    "        image_path = os.path.join(image_folder, image_files[0])  # Use the first image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Prepare report text for each anatomical region\n",
    "        region_reports = {region: report.get(region, \"\") for region in self.regions}\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'reports': region_reports,\n",
    "            'patient_id': patient_id\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        images = [item['image'] for item in batch]\n",
    "        reports = [item['reports'] for item in batch]\n",
    "        patient_ids = [item['patient_id'] for item in batch]\n",
    "\n",
    "        return {\n",
    "            'images': images,\n",
    "            'reports': reports,\n",
    "            'patient_ids': patient_ids\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "data_dir = 'data/images'\n",
    "annotation_file = 'data/annotation_quiz_all_with_val.json'\n",
    "\n",
    "train_dataset = XRayReportDataset(data_dir, annotation_file, split='train')\n",
    "val_dataset = XRayReportDataset(data_dir, annotation_file, split='val')\n",
    "test_dataset = XRayReportDataset(data_dir, annotation_file, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=XRayReportDataset.collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=XRayReportDataset.collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=XRayReportDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs function\n",
    "def prepare_inputs(batch, processor):\n",
    "    images = batch['images']\n",
    "    \n",
    "    conversations = []\n",
    "    for report in batch['reports']:\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"Provide a detailed X-ray report for the following anatomical regions: lung, heart, mediastinal, and bone.\"},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"Here's a detailed X-ray report for the anatomical regions:\\n\\nLung: {report['lung']}\\n\\nHeart: {report['heart']}\\n\\nMediastinal: {report['mediastinal']}\\n\\nBone: {report['bone']}\",\n",
    "            },\n",
    "        ]\n",
    "        conversations.append(conversation)\n",
    "    \n",
    "    inputs = processor(images=images, conversations=conversations, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Prepare labels for training\n",
    "    labels = inputs['labels'].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    inputs['labels'] = labels\n",
    "\n",
    "    return inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "light",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
